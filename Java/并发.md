
# JMM
## 什么是JMM
JMM是Java内存模型，描述了Java程序中各种变量(线程共享变量)的访问规则，以及在JVM中将变量，存储到内存和从内存中读取变量这样的底层细节。

![img](../img/主内存.png)
## 内存间交互操作
![img](../img/内存交互.png)
## JMM规定
所有的共享变量都存储于主内存，这里所说的变量指的是实例变量和类变量，不包含局部变量，因为局部变量是线程私有的，因此不存在竞争问题。

每一个线程还存在自己的工作内存，线程的工作内存，保留了被线程使用的变量的工作副本。工作内存存储在高速缓存或者寄存器中，保存了该线程使用的变量的主内存副本拷贝。

<font color=green>线程对变量的所有操作(读，取)都必须在工作内存中完成，而不能直接读写主内存的变量。 </font>

不同线程之间也不能直接访问对方工作内存中的变量，线程间变量的值的传递需要通过主内存中转来完成。

正是因为这样的机制，才导致了<font color=green>可见性问题</font>的存在。
# 内存模型三大特性
## 原子性
### 定义
一个操作或者多个操作，要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。

Java 内存模型保证了 read、load、use、assign、store、write、lock 和 unlock 操作具有原子性，例如对一个 int 类型的变量执行 assign 赋值操作，这个操作就是原子性的。但是 Java 内存模型允许虚拟机将没有被 volatile 修饰的 64 位数据（long，double）的读写操作划分为两次 32 位的操作来进行，即 load、store、read 和 write 操作可以不具备原子性。

### 保证原子性
- <font color=green>原子类</font>，如AtomicInteger 能保证多个线程修改的原子性。
- <font color=green>synchronized</font>，它对应的内存间交互操作为：lock 和 unlock，在虚拟机实现上对应的字节码指令为 monitorenter 和 monitorexi

## 可见性
### 定义
可见性指当一个线程修改了共享变量的值，其它线程能够立即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可见性的。
### 保证可见性
- 加锁，某一个线程进入synchronized代码块前后，线程会获得锁，清空工作内存，从主内存拷贝共享变量最新的值到工作内存成为副本，执行代码，将修改后的副本的值刷新回主内存中，线程释放锁。而获取不到锁的线程会阻塞等待，所以变量的值肯定一直都是最新的。
- volatile修饰共享变量。
- final，被 final 关键字修饰的字段在构造器中一旦初始化完成，并且没有发生 this 逃逸（其它线程通过 this 引用访问到初始化了一半的对象），那么其它线程就能看见 final 字段的值。

## 有序性
### 定义
有序性是指：在本线程内观察，所有操作都是有序的。在一个线程观察另一个线程，所有操作都是无序的，无序是因为发生了指令重排序。在 Java 内存模型中，允许编译器和处理器对指令进行重排序，重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。

```js
public class Singleton {
    private volatile static Singleton singleton;
    private Singleton() {
    }
    public static Singleton getSingleton() {
        if (singleton == null) {
            synchronized (Singleton.class) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }
}
```
在单例模式的双重校验锁中，singleton这个实例必须加volatile，防止指令重排，比如上面new对象的时候，经过了三个步骤，1.分配内存空间；
2调用构造器，初始化实例；3返回地址给引用，线程A可能的步骤是132，这没问题，但是此时如果有个线程B，在A执行1，3步骤后，在最初if判断里，由于在步骤3中已经把对象指向空间了，所以不等于null，所以线程B直接return singleton，但是它还没有完成构造。

### 保证有序性
- volatile 关键字通过添加内存屏障的方式来禁止指令重排，即重排序时不能把后面的指令放到内存屏障之前。
- 也可以通过 synchronized 来保证有序性，它保证每个时刻只有一个线程执行同步代码，相当于是让线程顺序执行同步代码。

# Volatile
Volatile保证不同线程对共享变量操作的可见性，每个线程操作数据的时候，会把数据从主内存读取到自己的工作内存，如果当前线程操作了数据并且写回了，其他已经读取的线程的变量副本就会失效了，需要重新去主内存读取。
## 保证可见性
### 缓存一致性协议
当多个处理器的运算任务都涉及同一块主内存区域时，将可能导致各自的缓存数据不一致，如果真的发生这种情况，那同步回到主内存时以谁的缓存数据为准呢？

为了解决一致性的问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作，这类协议有MSI、MESI（IllinoisProtocol）、MOSI、Synapse、Firefly及DragonProtocol等。

#### MESI
当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。

### 如何发现数据是否失效
#### 嗅探
每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。

### 通过Volatile保证可见性的问题
由于Volatile的MESI缓存一致性协议，需要不断的从主内存嗅探和cas不断循环，无效交互会导致总线带宽达到峰值。

所以不要大量使用Volatile，至于什么时候去使用Volatile什么时候使用锁，根据场景区分。
## 保证有序性
java编译器会在生成指令系列时在适当的位置会插入内存屏障指令来禁止特定类型的处理器重排序。

需要注意的是：volatile写是在前面和后面分别插入内存屏障，而volatile读操作是在后面插入两个内存屏障。

## Volatile和Synchronized的区别
volatile只能修饰实例变量和类变量，而synchronized可以修饰方法，以及代码块。

volatile保证数据的可见性，但是不保证原子性(多线程进行写操作，不保证线程安全);而synchronized是一种排他(互斥)的机制。

volatile用于禁止指令重排序：可以解决单例双重检查对象初始化代码执行乱序问题。

volatile可以看做是轻量版的synchronized，volatile不保证原子性，但是如果是对一个共享变量进行多个线程的赋值，而没有其他的操作，那么就可以用volatile来代替synchronized，因为赋值本身是有原子性的，而volatile又保证了可见性，所以就可以保证线程安全了。

## 总结
1. volatile修饰符适用于以下场景：某个属性被多个线程共享，其中有一个线程修改了此属性，其他线程可以立即得到修改后的值，比如booleanflag(如图);或者作为触发器，实现轻量级同步。
2. volatile属性的读写操作都是无锁的，它不能替代synchronized，因为它没有提供原子性和互斥性。因为无锁，不需要花费时间在获取锁和释放锁上，所以说它是低成本的。
3. volatile只能作用于属性，我们用volatile修饰属性，这样compilers就不会对这个属性做指令重排序。
4. volatile提供了可见性，任何一个线程对其的修改将立马对其他线程可见，volatile属性不会被线程缓存，始终从主 存中读取。
5. volatile提供了happens-before保证，对volatile变量v的写入happens-before所有其他线程后续对v的读操作。
6. volatile可以使得long和double的赋值是原子的。
7. volatile可以在单例双重检查中实现可见性和禁止指令重排序，从而保证安全性。

![img](../img/Volatile代码.png)

# AQS
[详情](https://mp.weixin.qq.com/s/hB5ncpe7_tVovQj1sNlDRA)

## 什么是AQS
AQS是一个抽象类。

AQS内部维护一个state状态位，尝试加锁的时候通过CAS(CompareAndSwap)修改值，如果成功设置为1，并且把当前线程ID赋值，则代表加锁成功，一旦获取到锁，其他的线程将会被阻塞进入阻塞队列自旋，获得锁的线程释放锁的时候将会唤醒阻塞队列中的线程，释放锁的时候则会把state重新置为0，同时当前线程ID置为空。

## AQS内部数据结构

 在AbstractQueuedSynchronizer内部，有一个队列，我们把它叫做同步等待队列。它的作用是保存等待在这个锁上的线程(由于lock()操作引起的等待）。此外，为了维护等待在条件变量上的等待线程，AbstractQueuedSynchronizer又需要再维护一个条件变量等待队列，也就是那些由Condition.await()引起阻塞的线程。

 ![img](../img/condition1.png)
 ![img](../img/condition2.png)

 由于一个重入锁可以生成多个条件变量对象，因此，一个重入锁就可能有多个条件变量等待队列。实际上，每个条件变量对象内部都维护了一个等待列表.

 ![img](../img/lock.png)
 ![img](../img/aqs.png)

可以看到，无论是同步等待队列，还是条件变量等待队列，都使用同一个Node类作为链表的节点。对于同步等待队列，Node中包括链表的上一个元素prev，下一个元素next和线程对象thread。对于条件变量等待队列，还使用nextWaiter表示下一个等待在条件变量队列中的节点。

Node节点另外一个重要的成员是waitStatus，它表示节点等待在队列中的状态：

- CANCELLED：表示线程取消了等待。如果取得锁的过程中发生了一些异常，则可能出现取消的情况，比如等待过程中出现了中断异常或者出现了timeout。
- SIGNAL：表示后续节点需要被唤醒。
- CONDITION：线程等待在条件变量队列中。
- PROPAGATE：在共享模式下，无条件传播。
- releaseShared状态。早期的JDK并没有这个状态，咋看之下，这个状态是多余的。引入这个状态是为了解决共享锁并发释放引起线程挂起的bug 6801020。（随着JDK的不断完善，它的代码也越来越难懂了 :(，就和我们自己的工程代码一样，bug修多了，细节就显得越来越晦涩）
- 0：初始状态
其中CANCELLED=1，SIGNAL=-1，CONDITION=-2，PROPAGATE=-3 。在具体的实现中，就可以简单的通过waitStatus释放小于等于0，来判断是否是CANCELLED状态。

## 排它锁实现
![img](../img/排它锁.png)

进入一步看一下tryAcquire()函数。该函数的作用是尝试获得一个许可。对于AbstractQueuedSynchronizer来说，这是一个未实现的抽象函数。

具体实现在子类中。在重入锁，读写锁，信号量等实现中， 都有各自的实现。

如果tryAcquire()成功，则acquire()直接返回成功。如果失败，就用addWaiter()将当前线程加入同步等待队列。

![img](../img/addwaiter.png)

接着， 对已经在队列中的线程请求锁，使用acquireQueued()函数，从函数名字上可以看到，其参数node，必须是一个已经在队列中等待的节点。它的功能就是为已经在队列中的Node请求一个许可。

![img](../img/acquire.png)

### Condition.await()
如果调用Condition.await()，那么线程也会进入等待，下面来看实现：
![img](../img/await.png)
### signal()
signal()通知的时候，是在条件等待队列中，按照FIFO进行，首先从第一个节点下手:
![img](../img/signal.png)
### release()
释放排他锁:

![img](../img/release.png)

## 共享锁
与排他锁相比，共享锁的实现略微复杂一点。这也很好理解。因为排他锁的场景很简单，单进单出，而共享锁就不一样了。可能是N进M出，处理起来要麻烦一些。但是，他们的核心思想还是一致的。共享锁的几个典型应用有：信号量，读写锁中的写锁。

### 获得共享锁
为了实现共享锁，在AbstractQueuedSynchronizer中，专门有一套针对共享锁的方法。

获得共享锁使用acquireShared()方法

![img](../img/shareacquire1.png)
![img](../img/shareacquire2.png)

### 释放共享锁
![img](../img/releaseshare.png)

# 排它锁-ReentrantLock
## 结构
ReentrantLock内有一个Sync实例，Sync继承了AQS，所以ReentrantLock使用的lock方法其实就是调用了Sync的lock方法

![img](../img/reentrantlock1.png)

默认是非公平锁

![img](../img/reentrantlock2.png)

## 非公平锁

![img](../img/nonfair.png)
![img](../img/nonfairtryacquire.png)

## 公平锁
![img](../img/fairlock.png)

## 使用
![img](../img/lockuse.png)

# CAS
[详情](https://mp.weixin.qq.com/s/WtAdXvaRuBZ-SXayIKu1mA)

CAS叫做CompareAndSwap，比较并交换，主要是通过处理器的指令来保证操作的原子性，它包含三个操作数：

> 1. 变量内存地址，V表示
> 2. 旧的预期值，A表示
> 3. 准备设置的新值，B表示

## CAS 是怎么实现线程安全的？
线程在读取数据时不进行加锁，在准备写回数据时，先去查询原值，操作的时候比较原值是否修改，若未被其他线程修改则写回，若已被修改，则重新执行读取流程。
## CAS缺点
CAS的缺点主要有3点：
> 1. ABA问题：ABA的问题指的是在CAS更新的过程中，当读取到的值是A，然后准备赋值的时候仍然是A，但是实际上有可能A的值被改成了B，然后又被改回了A，这个CAS更新的漏洞就叫做ABA。只是ABA的问题大部分场景下都不影响并发的最终效果。
> 
> Java中有AtomicStampedReference来解决这个问题，他加入了预期标志和更新后标志两个字段，更新时不光检查值，还要检查当前的标志是否等于预期标志，全部相等的话才会更新。
> 
> 2. 循环时间长开销大：自旋CAS的方式如果长时间不成功，会给CPU带来很大的开销。
> 
> 3. 只能保证一个共享变量的原子操作：只对一个共享变量操作可以保证原子性，但是多个则不行，多个可以通过AtomicReference来处理或者使用锁synchronized实现。


# ThreadLocal

```
ThreadLocal<String> localName = new ThreadLocal();
localName.set("张三");
String name = localName.get();
localName.remove();
```
> 其实使用真的很简单，线程进来之后初始化一个可以泛型的ThreadLocal对象，之后这个线程只要在remove之前去get，都能拿到之前set的值，注意这里我说的是remove之前。

## ThreadLocal结构
ThreadLocal类提供set/get方法存储和获取value值，但实际上ThreadLocal类并不存储value值，真正存储是靠ThreadLocalMap这个类，ThreadLocalMap是ThreadLocal的一个静态内部类，它的key是ThreadLocal实例对象，value是任意Object对象。

## ThreadLocal例子
> Spring采用Threadlocal的方式，来保证单个线程中的数据库操作使用的是同一个数据库连接，同时，采用这种方式可以使业务层使用事务时不需要感知并管理connection对象，通过传播级别，巧妙地管理多个事务配置之间的切换，挂起和恢复。
## ThreadLocal内存泄漏问题
ThreadLocalMap中使用的key为ThreadLocal的弱引用，而value是强引用。所以如果ThreadLocal没有被外部强引用的情况下，在垃圾回收时，key会被清理掉，而value不会被清理掉。这样一来，ThreadLocalMap中就会出现key为null的Entry。假如我们不做任何措施的话，value永远无法被GC回收，这个时候就可能会产生内存泄漏。所以使用完ThreadLocal后，记得调用remove()。

## 内存泄漏和内存溢出
内存泄漏
> 指程序中动态分配内存给一些临时对象，但是对象不会被GC所回收，始终占用内存。

内存溢出
> 指程序运行过程中无法申请到足够的内存而导致的一种错误。(OOM)

# 线程池 

## 原理
其实java线程池的实现原理很简单，说白了就是一个线程集合workerSet和一个阻塞队列workQueue。当用户向线程池提交一个任务(也就是线程)时，线程池会先将任务放入workQueue中。workerSet中的线程会不断的从workQueue中获取线程然后执行。当workQueue中没有任务的时候，worker就会阻塞，直到队列中有任务了就取出来继续执行。

## 核心参数
> 1. 核心线程数 corePoolSize
> 2. 最大线程数 maximumPoolSize
> 3. 活跃时间 keepAliveTime
> 4. 阻塞队列 workQueue
> 5. 拒绝策略 RejectedExecutionHandler

## 提交一个新任务到线程池时，具体执行流程：
> 1. 当我们提交任务，线程池会根据corePoolSize大小创建若干任务线程执行任务
> 2. 当任务的数量超过corePoolSize时，后续的任务将会进入阻塞队列阻塞排队
> 3. 当阻塞队列也满了的时候，将会继续创建(maximumPoolSize-corePoolSize)个数量的线程执行任务，如果任务处理完成，maximumPoolSize-corePoolSize额外创建的线程等待keepAliveTime之后被自动销毁
> 4. 如果达到maximumPoolSize，阻塞队列还是满的状态，那么将根据不同的拒绝策略对应处理

## 主要有4种拒绝策略
> 1. AbortPolicy：直接丢弃任务，抛出异常，这是默认策略
> 2. CallerRunsPolicy：只用调用者所在的线程来处理任务
> 3. DiscardOldestPolicy：丢弃等待队列中最旧的任务，并执行当前任务
> 4. DiscardPolicy：直接丢弃任务，也不抛出异常

## 使用线程池经验
1. 通过一个线程池检查在线测评机，如果测评机掉线了，就把它从map中删掉
![img](../img/线程池使用1.png)

2. 创建线程池来处理用户提交的判题任务。

corepoolsize =  cpu核数

maxpoolsize = corepoolsize*2

## 参考
tasks ：每秒的任务数，假设为500~1000

taskcost：每个任务花费时间，假设为0.1s

responsetime：系统允许容忍的最大响应时间，假设为1s

corePoolSize = 每秒需要多少个线程处理？ 

threadcount = tasks/(1/taskcost) =tasks*taskcout =  (500~1000)*0.1 = 50~100 个线程。corePoolSize设置应该大于50

根据8020原则，如果80%的每秒任务数小于800，那么corePoolSize设置为80即可

queueCapacity = (coreSizePool/taskcost)*responsetime

计算可得 queueCapacity = 80/0.1*1 = 80。意思是队列里的线程可以等待1s，超过了的需要新开线程来执行

切记不能设置为Integer.MAX_VALUE，这样队列会很大，线程数只会保持在corePoolSize大小，当任务陡增时，不能新开线程来执行，响应时间会随之陡增。

maxPoolSize = (max(tasks)- queueCapacity)/(1/taskcost)

计算可得 maxPoolSize = (1000-80)/10 = 92

（最大任务数-队列容量）/每个线程每秒处理能力 = 最大线程数



## 阿里JAVA开发手册规定不能用

```
ExecutorService executorService = Executors.newCachedThreadPool();
```
### 弊端
> - FixedThreadPool和SingleThreadExecutor允许请求的队列长度为maxValue，可能堆积大量的请求，导致OOM
> - CachedThreadPool和ScheduledThreadPool允许创建的线程数量为maxValue，可能会创建大量的线程，导致OOM

## 业务实践
### 1. 快速响应用户请求
> - 描述：用户发起的实时请求，服务追求响应时间。比如说用户要查看一个商品的信息，那么我们需要将商品维度的一系列信息如商品的价格、优惠、库存、图片等等聚合起来，展示给用户。
>
> - 分析：从用户体验角度看，这个结果响应的越快越好，如果一个页面半天都刷不出，用户可能就放弃查看这个商品了。而面向用户的功能聚合通常非常复杂，伴随着调用与调用之间的级联、多级级联等情况，业务开发同学往往会选择使用线程池这种简单的方式，将调用封装成任务并行的执行，缩短总体响应时间。另外，使用线程池也是有考量的，这种场景最重要的就是获取最大的响应速度去满足用户，所以应该不设置队列去缓冲并发任务，调高corePoolSize和maxPoolSize去尽可能创造多的线程快速执行任务。

### 2. 快速处理批量任务
> - 描述：离线的大量计算任务，需要快速执行。比如说，统计某个报表，需要计算出全国各个门店中有哪些商品有某种属性，用于后续营销策略的分析，那么我们需要查询全国所有门店中的所有商品，并且记录具有某属性的商品，然后快速生成报表。
>
> - 分析：这种场景需要执行大量的任务，我们也会希望任务执行的越快越好。这种情况下，也应该使用多线程策略，并行计算。但与响应速度优先的场景区别在于，这类场景任务量巨大，并不需要瞬时的完成，而是关注如何使用有限的资源，尽可能在单位时间内处理更多的任务，也就是吞吐量优先的问题。所以应该设置队列去缓冲并发任务，调整合适的corePoolSize去设置处理任务的线程数。在这里，设置的线程数过多可能还会引发线程上下文切换频繁的问题，也会降低处理任务的速度，降低吞吐量。


# 死锁
```java
public class DeadLockDemo {
    private static Object resource1 = new Object();//资源 1 
    private static Object resource2 = new Object();//资源 2

    public static void main(String[] args) {
        new Thread(()->{
            synchronized(resource1){
                System.out.println(Thread.currentThread() + "get resource1");
                try{
                    Thread.sleep(1000);
                } catch(InterruptedException e){
                    e.printStackTrace();
                }
                System.out.println(Thread.currentThread() + "waiting get resource2");
                synchronized(resource2){
                    System.out.println(Thread.currentThread() + "get resource2");
                }
            }
        }, "Thread 1").start();

        new Thread(()->{
            synchronized(resource2){
                System.out.println(Thread.currentThread() + "get resource2");
                try{
                    Thread.sleep(1000);
                } catch(InterruptedException e){
                    e.printStackTrace();
                }
                System.out.println(Thread.currentThread() + "waiting get resource1");
                synchronized(resource1){
                    System.out.println(Thread.currentThread() + "get resource1");
                }
            }
        }, "Thread 2").start();
    }
}
```
output
```
Thread[线程 1,5,main]get resource1
Thread[线程 2,5,main]get resource2
Thread[线程 1,5,main]waiting get resource2
Thread[线程 2,5,main]waiting get resource1
```
 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过 Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视 器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等
待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。

解决
```java
// 对线程2代码修改
new Thread(() -> {
    synchronized (resource1) {
        System.out.println(Thread.currentThread() + "get resource1");
        try {
            Thread.sleep(1000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println(Thread.currentThread() + "waiting get resource2");
        synchronized (resource2) {
            System.out.println(Thread.currentThread() + "get resource2");
        }
}
```
