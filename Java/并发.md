
# JMM
## 什么是JMM
JMM是Java内存模型，描述了Java程序中各种变量(线程共享变量)的访问规则，以及在JVM中将变量，存储到内存和从内存中读取变量这样的底层细节。

![img](../img/主内存.png)
## 内存间交互操作
![img](../img/内存交互.png)
## JMM规定
所有的共享变量都存储于主内存，这里所说的变量指的是实例变量和类变量，不包含局部变量，因为局部变量是线程私有的，因此不存在竞争问题。

每一个线程还存在自己的工作内存，线程的工作内存，保留了被线程使用的变量的工作副本。工作内存存储在高速缓存或者寄存器中，保存了该线程使用的变量的主内存副本拷贝。

<font color=green>线程对变量的所有操作(读，取)都必须在工作内存中完成，而不能直接读写主内存的变量。 </font>

不同线程之间也不能直接访问对方工作内存中的变量，线程间变量的值的传递需要通过主内存中转来完成。

正是因为这样的机制，才导致了<font color=green>可见性问题</font>的存在。
# 内存模型三大特性
## 原子性
### 定义
一个操作或者多个操作，要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。

Java 内存模型保证了 read、load、use、assign、store、write、lock 和 unlock 操作具有原子性，例如对一个 int 类型的变量执行 assign 赋值操作，这个操作就是原子性的。但是 Java 内存模型允许虚拟机将没有被 volatile 修饰的 64 位数据（long，double）的读写操作划分为两次 32 位的操作来进行，即 load、store、read 和 write 操作可以不具备原子性。

### 保证原子性
- <font color=green>原子类</font>，如AtomicInteger 能保证多个线程修改的原子性。
- <font color=green>synchronized</font>，它对应的内存间交互操作为：lock 和 unlock，在虚拟机实现上对应的字节码指令为 monitorenter 和 monitorexi

## 可见性
### 定义
可见性指当一个线程修改了共享变量的值，其它线程能够立即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可见性的。
### 保证可见性
- 加锁，某一个线程进入synchronized代码块前后，线程会获得锁，清空工作内存，从主内存拷贝共享变量最新的值到工作内存成为副本，执行代码，将修改后的副本的值刷新回主内存中，线程释放锁。而获取不到锁的线程会阻塞等待，所以变量的值肯定一直都是最新的。
- volatile修饰共享变量。
- final，被 final 关键字修饰的字段在构造器中一旦初始化完成，并且没有发生 this 逃逸（其它线程通过 this 引用访问到初始化了一半的对象），那么其它线程就能看见 final 字段的值。

## 有序性
### 定义
有序性是指：在本线程内观察，所有操作都是有序的。在一个线程观察另一个线程，所有操作都是无序的，无序是因为发生了指令重排序。在 Java 内存模型中，允许编译器和处理器对指令进行重排序，重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。

```java
public class Singleton {
    private volatile static Singleton singleton;
    private Singleton() {
    }
    public static Singleton getSingleton() {
        if (singleton == null) {
            synchronized (Singleton.class) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }
}
```
在单例模式的双重校验锁中，singleton这个实例必须加volatile，防止指令重排，比如上面new对象的时候，经过了三个步骤，1.分配内存空间；
2调用构造器，初始化实例；3返回地址给引用，线程A可能的步骤是132，这没问题，但是此时如果有个线程B，在A执行1，3步骤后，在最初if判断里，由于在步骤3中已经把对象指向空间了，所以不等于null，所以线程B直接return singleton，但是它还没有完成构造。

### 保证有序性
- volatile 关键字通过添加内存屏障的方式来禁止指令重排，即重排序时不能把后面的指令放到内存屏障之前。
- 也可以通过 synchronized 来保证有序性，它保证每个时刻只有一个线程执行同步代码，相当于是让线程顺序执行同步代码。

# Volatile
Volatile保证不同线程对共享变量操作的可见性，每个线程操作数据的时候，会把数据从主内存读取到自己的工作内存，如果当前线程操作了数据并且写回了，其他已经读取的线程的变量副本就会失效了，需要重新去主内存读取。
## 保证可见性
### 缓存一致性协议
当多个处理器的运算任务都涉及同一块主内存区域时，将可能导致各自的缓存数据不一致，如果真的发生这种情况，那同步回到主内存时以谁的缓存数据为准呢？

为了解决一致性的问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作，这类协议有MSI、MESI（IllinoisProtocol）、MOSI、Synapse、Firefly及DragonProtocol等。

#### MESI
当CPU写数据时，如果发现操作的变量是共享变量，即在其他CPU中也存在该变量的副本，会发出信号通知其他CPU将该变量的缓存行置为无效状态，因此当其他CPU需要读取这个变量时，发现自己缓存中缓存该变量的缓存行是无效的，那么它就会从内存重新读取。

### 如何发现数据是否失效
#### 嗅探
每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。

### 通过Volatile保证可见性的问题
由于Volatile的MESI缓存一致性协议，需要不断的从主内存嗅探和cas不断循环，无效交互会导致总线带宽达到峰值。

所以不要大量使用Volatile，至于什么时候去使用Volatile什么时候使用锁，根据场景区分。
## 保证有序性
java编译器会在生成指令系列时在适当的位置会插入内存屏障指令来禁止特定类型的处理器重排序。

需要注意的是：volatile写是在前面和后面分别插入内存屏障，而volatile读操作是在后面插入两个内存屏障。

## Volatile和Synchronized的区别
volatile只能修饰实例变量和类变量，而synchronized可以修饰方法，以及代码块。

volatile保证数据的可见性，但是不保证原子性(多线程进行写操作，不保证线程安全);而synchronized是一种排他(互斥)的机制。

volatile用于禁止指令重排序：可以解决单例双重检查对象初始化代码执行乱序问题。

volatile可以看做是轻量版的synchronized，volatile不保证原子性，但是如果是对一个共享变量进行多个线程的赋值，而没有其他的操作，那么就可以用volatile来代替synchronized，因为赋值本身是有原子性的，而volatile又保证了可见性，所以就可以保证线程安全了。

## 总结
1. volatile修饰符适用于以下场景：某个属性被多个线程共享，其中有一个线程修改了此属性，其他线程可以立即得到修改后的值，比如booleanflag(如图);或者作为触发器，实现轻量级同步。
2. volatile属性的读写操作都是无锁的，它不能替代synchronized，因为它没有提供原子性和互斥性。因为无锁，不需要花费时间在获取锁和释放锁上，所以说它是低成本的。
3. volatile只能作用于属性，我们用volatile修饰属性，这样compilers就不会对这个属性做指令重排序。
4. volatile提供了可见性，任何一个线程对其的修改将立马对其他线程可见，volatile属性不会被线程缓存，始终从主 存中读取。
5. volatile提供了happens-before保证，对volatile变量v的写入happens-before所有其他线程后续对v的读操作。
6. volatile可以使得long和double的赋值是原子的。
7. volatile可以在单例双重检查中实现可见性和禁止指令重排序，从而保证安全性。

![img](../img/Volatile代码.png)

# AQS
[详情](https://mp.weixin.qq.com/s/hB5ncpe7_tVovQj1sNlDRA)

## 什么是AQS
AQS是一个抽象类。

AQS内部维护一个state状态位，尝试加锁的时候通过CAS(CompareAndSwap)修改值，如果成功设置为1，并且把当前线程ID赋值，则代表加锁成功，一旦获取到锁，其他的线程将会被阻塞进入阻塞队列自旋，获得锁的线程释放锁的时候将会唤醒阻塞队列中的线程，释放锁的时候则会把state重新置为0，同时当前线程ID置为空。

## AQS内部数据结构

 在AbstractQueuedSynchronizer内部，有一个队列，我们把它叫做同步等待队列。它的作用是保存等待在这个锁上的线程(由于lock()操作引起的等待）。此外，为了维护等待在条件变量上的等待线程，AbstractQueuedSynchronizer又需要再维护一个条件变量等待队列，也就是那些由Condition.await()引起阻塞的线程。

 ![img](../img/condition1.png)
 ![img](../img/condition2.png)

 由于一个重入锁可以生成多个条件变量对象，因此，一个重入锁就可能有多个条件变量等待队列。实际上，每个条件变量对象内部都维护了一个等待列表.

 ![img](../img/lock.png)
 ![img](../img/aqs.png)

可以看到，无论是同步等待队列，还是条件变量等待队列，都使用同一个Node类作为链表的节点。对于同步等待队列，Node中包括链表的上一个元素prev，下一个元素next和线程对象thread。对于条件变量等待队列，还使用nextWaiter表示下一个等待在条件变量队列中的节点。

Node节点另外一个重要的成员是waitStatus，它表示节点等待在队列中的状态：

- CANCELLED：表示线程取消了等待。如果取得锁的过程中发生了一些异常，则可能出现取消的情况，比如等待过程中出现了中断异常或者出现了timeout。
- SIGNAL：表示后续节点需要被唤醒。
- CONDITION：线程等待在条件变量队列中。
- PROPAGATE：在共享模式下，无条件传播。
- releaseShared状态。早期的JDK并没有这个状态，咋看之下，这个状态是多余的。引入这个状态是为了解决共享锁并发释放引起线程挂起的bug 6801020。（随着JDK的不断完善，它的代码也越来越难懂了 :(，就和我们自己的工程代码一样，bug修多了，细节就显得越来越晦涩）
- 0：初始状态
其中CANCELLED=1，SIGNAL=-1，CONDITION=-2，PROPAGATE=-3 。在具体的实现中，就可以简单的通过waitStatus释放小于等于0，来判断是否是CANCELLED状态。

## 排它锁实现
![img](../img/排它锁.png)

进入一步看一下tryAcquire()函数。该函数的作用是尝试获得一个许可。对于AbstractQueuedSynchronizer来说，这是一个未实现的抽象函数。

具体实现在子类中。在重入锁，读写锁，信号量等实现中， 都有各自的实现。

如果tryAcquire()成功，则acquire()直接返回成功。如果失败，就用addWaiter()将当前线程加入同步等待队列。

![img](../img/addwaiter.png)

接着， 对已经在队列中的线程请求锁，使用acquireQueued()函数，从函数名字上可以看到，其参数node，必须是一个已经在队列中等待的节点。它的功能就是为已经在队列中的Node请求一个许可。

![img](../img/acquire.png)

### Condition.await()
如果调用Condition.await()，那么线程也会进入等待，下面来看实现：
![img](../img/await.png)
### signal()
signal()通知的时候，是在条件等待队列中，按照FIFO进行，首先从第一个节点下手:
![img](../img/signal.png)
### release()
释放排他锁:

![img](../img/release.png)

## 共享锁
与排他锁相比，共享锁的实现略微复杂一点。这也很好理解。因为排他锁的场景很简单，单进单出，而共享锁就不一样了。可能是N进M出，处理起来要麻烦一些。但是，他们的核心思想还是一致的。共享锁的几个典型应用有：信号量，读写锁中的写锁。

### 获得共享锁
为了实现共享锁，在AbstractQueuedSynchronizer中，专门有一套针对共享锁的方法。

获得共享锁使用acquireShared()方法

![img](../img/shareacquire1.png)
![img](../img/shareacquire2.png)

### 释放共享锁
![img](../img/releaseshare.png)

# 排它锁-ReentrantLock
## 结构
ReentrantLock内有一个Sync实例，Sync继承了AQS，所以ReentrantLock使用的lock方法其实就是调用了Sync的lock方法

![img](../img/reentrantlock1.png)

默认是非公平锁

![img](../img/reentrantlock2.png)

## 非公平锁

![img](../img/nonfair.png)
![img](../img/nonfairtryacquire.png)

## 公平锁
![img](../img/fairlock.png)

## 使用
![img](../img/lockuse.png)

# CAS
[详情](https://mp.weixin.qq.com/s/WtAdXvaRuBZ-SXayIKu1mA)

CAS叫做CompareAndSwap，比较并交换，主要是通过处理器的指令来保证操作的原子性，它包含三个操作数：

> 1. 变量内存地址，V表示
> 2. 旧的预期值，A表示
> 3. 准备设置的新值，B表示

## CAS 是怎么实现线程安全的？
线程在读取数据时不进行加锁，在准备写回数据时，先去查询原值，操作的时候比较原值是否修改，若未被其他线程修改则写回，若已被修改，则重新执行读取流程。
## CAS缺点
CAS的缺点主要有3点：
> 1. ABA问题：ABA的问题指的是在CAS更新的过程中，当读取到的值是A，然后准备赋值的时候仍然是A，但是实际上有可能A的值被改成了B，然后又被改回了A，这个CAS更新的漏洞就叫做ABA。只是ABA的问题大部分场景下都不影响并发的最终效果。
> 
> Java中有AtomicStampedReference来解决这个问题，他加入了预期标志和更新后标志两个字段，更新时不光检查值，还要检查当前的标志是否等于预期标志，全部相等的话才会更新。
> 
> 2. 循环时间长开销大：自旋CAS的方式如果长时间不成功，会给CPU带来很大的开销。
> 
> 3. 只能保证一个共享变量的原子操作：只对一个共享变量操作可以保证原子性，但是多个则不行，多个可以通过AtomicReference来处理或者使用锁synchronized实现。


# ThreadLocal

```
ThreadLocal<String> localName = new ThreadLocal();
localName.set("张三");
String name = localName.get();
localName.remove();
```
> 其实使用真的很简单，线程进来之后初始化一个可以泛型的ThreadLocal对象，之后这个线程只要在remove之前去get，都能拿到之前set的值，注意这里我说的是remove之前。

## ThreadLocal结构
ThreadLocal类提供set/get方法存储和获取value值，但实际上ThreadLocal类并不存储value值，真正存储是靠ThreadLocalMap这个类，ThreadLocalMap是ThreadLocal的一个静态内部类，它的key是ThreadLocal实例对象，value是任意Object对象。

## ThreadLocal例子
> Spring采用Threadlocal的方式，来保证单个线程中的数据库操作使用的是同一个数据库连接，同时，采用这种方式可以使业务层使用事务时不需要感知并管理connection对象，通过传播级别，巧妙地管理多个事务配置之间的切换，挂起和恢复。
## ThreadLocal内存泄漏问题
ThreadLocalMap中使用的key为ThreadLocal的弱引用，而value是强引用。所以如果ThreadLocal没有被外部强引用的情况下，在垃圾回收时，key会被清理掉，而value不会被清理掉。这样一来，ThreadLocalMap中就会出现key为null的Entry。假如我们不做任何措施的话，value永远无法被GC回收，这个时候就可能会产生内存泄漏。所以使用完ThreadLocal后，记得调用remove()。

## 内存泄漏和内存溢出
内存泄漏
> 指程序中动态分配内存给一些临时对象，但是对象不会被GC所回收，始终占用内存。

内存溢出
> 指程序运行过程中无法申请到足够的内存而导致的一种错误。(OOM)

## 如何跨线程传递ThreadLocal的值
**应用场景**
- 日志追踪：分布式系统常用ThreadLocal存储traceId，异步/子线程也需要打上同一个traceId
- 用户上下文：主线程存有当前用户信息，异步线程也需要该用户信息

### InheritableThreadLocal
由jdk1.2提供

Thread中有inheritableThreadLocals变量

在创建线程时，会获取父线程，拿到父线程的 inheritableThreadLocals赋值给自己的inheritableThreadLocals
``` java
// Thread 的构造方法会调用 init() 方法
private void init(/* ... */) {
	// 1、获取父线程
    Thread parent = currentThread();
    // 2、将父线程的 inheritableThreadLocals 赋值给子线程
    if (inheritThreadLocals && parent.inheritableThreadLocals != null)
        this.inheritableThreadLocals =
        	ThreadLocal.createInheritedMap(parent.inheritableThreadLocals);
}
```
### TransmittableThreadLocal
JDK默认不支持线程池场景下ThreadLocal值传递，所以阿里开源了该工具

主要改造点：
- 实现自定义的Thread，在run方法中做ThreadLocal变量的赋值操作
- 基于线程池进行装饰，在execute方法中，不提交JDK的Thread，而是提交自定义的Thread


# 线程池 

## 原理
其实java线程池的实现原理很简单，说白了就是一个线程集合workerSet和一个阻塞队列workQueue。当用户向线程池提交一个任务(也就是线程)时，线程池会先将任务放入workQueue中。workerSet中的线程会不断的从workQueue中获取线程然后执行。当workQueue中没有任务的时候，worker就会阻塞，直到队列中有任务了就取出来继续执行。

1. 如果当前工作线程数量小于 corePoolSize，线程池会创建一个新的 Worker；

2. 提交的任务会作为 firstTask 传入 Worker 构造方法；

3. 随后，线程池启动该 Worker 封装的线程，线程会调用 Worker.run()；

4. Worker.run() 会进一步调用 runWorker(this)： 
   - 首先执行 firstTask.run()； 
   - 然后进入循环，从 workQueue 中不断获取并执行后续任务；

5. 线程将持续运行，直到没有任务或线程池关闭/线程中断。

## 创建
![img](../img/并发/image-20250731224441371.png)

## 核心参数
> 1. 核心线程数 corePoolSize
> 2. 最大线程数 maximumPoolSize
> 3. 活跃时间 keepAliveTime
> 4. 阻塞队列 workQueue
> 5. 拒绝策略 RejectedExecutionHandler

## 提交一个新任务到线程池时，具体执行流程：
> 1. 当我们提交任务，线程池会根据corePoolSize大小创建若干任务线程执行任务
> 2. 当任务的数量超过corePoolSize时，后续的任务将会进入阻塞队列阻塞排队
> 3. 当阻塞队列也满了的时候，将会继续创建(maximumPoolSize-corePoolSize)个数量的线程执行任务，如果任务处理完成，maximumPoolSize-corePoolSize额外创建的线程等待keepAliveTime之后被自动销毁
> 4. 如果达到maximumPoolSize，阻塞队列还是满的状态，那么将根据不同的拒绝策略对应处理

## 主要有4种拒绝策略
> 1. AbortPolicy：直接丢弃任务，抛出异常，这是默认策略
> 2. CallerRunsPolicy：只用调用者所在的线程来处理任务
> 3. DiscardOldestPolicy：丢弃等待队列中最旧的任务，并执行当前任务
> 4. DiscardPolicy：直接丢弃任务，也不抛出异常

## 使用线程池经验
1. 通过一个线程池检查在线测评机，如果测评机掉线了，就把它从map中删掉
![img](../img/线程池使用1.png)

2. 创建线程池来处理用户提交的判题任务。

corepoolsize =  cpu核数

maxpoolsize = corepoolsize*2

## 参考
tasks ：每秒的任务数，假设为500~1000

taskcost：每个任务花费时间，假设为0.1s

responsetime：系统允许容忍的最大响应时间，假设为1s

corePoolSize = 每秒需要多少个线程处理？ 

threadcount = tasks/(1/taskcost) =tasks * taskcout =  (500~1000)*0.1 = 50~100 个线程。corePoolSize设置应该大于50

根据8020原则，如果80%的每秒任务数小于800，那么corePoolSize设置为80即可

queueCapacity = (coreSizePool/taskcost)*responsetime

计算可得 queueCapacity = 80/0.1*1 = 80。意思是队列里的线程可以等待1s，超过了的需要新开线程来执行

切记不能设置为Integer.MAX_VALUE，这样队列会很大，线程数只会保持在corePoolSize大小，当任务陡增时，不能新开线程来执行，响应时间会随之陡增。

maxPoolSize = (max(tasks)- queueCapacity)/(1/taskcost)

计算可得 maxPoolSize = (1000-80)/10 = 92

（最大任务数-队列容量）/每个线程每秒处理能力 = 最大线程数



## 阿里JAVA开发手册规定不能用

```
ExecutorService executorService = Executors.newCachedThreadPool();
```
### 弊端
> - FixedThreadPool和SingleThreadExecutor允许请求的队列长度为maxValue，可能堆积大量的请求，导致OOM
> - CachedThreadPool和ScheduledThreadPool允许创建的线程数量为maxValue，可能会创建大量的线程，导致OOM

## 业务实践
### 1. 快速响应用户请求
> - 描述：用户发起的实时请求，服务追求响应时间。比如说用户要查看一个商品的信息，那么我们需要将商品维度的一系列信息如商品的价格、优惠、库存、图片等等聚合起来，展示给用户。
>
> - 分析：从用户体验角度看，这个结果响应的越快越好，如果一个页面半天都刷不出，用户可能就放弃查看这个商品了。而面向用户的功能聚合通常非常复杂，伴随着调用与调用之间的级联、多级级联等情况，业务开发同学往往会选择使用线程池这种简单的方式，将调用封装成任务并行的执行，缩短总体响应时间。另外，使用线程池也是有考量的，这种场景最重要的就是获取最大的响应速度去满足用户，所以应该不设置队列去缓冲并发任务，调高corePoolSize和maxPoolSize去尽可能创造多的线程快速执行任务。

### 2. 快速处理批量任务
> - 描述：离线的大量计算任务，需要快速执行。比如说，统计某个报表，需要计算出全国各个门店中有哪些商品有某种属性，用于后续营销策略的分析，那么我们需要查询全国所有门店中的所有商品，并且记录具有某属性的商品，然后快速生成报表。
>
> - 分析：这种场景需要执行大量的任务，我们也会希望任务执行的越快越好。这种情况下，也应该使用多线程策略，并行计算。但与响应速度优先的场景区别在于，这类场景任务量巨大，并不需要瞬时的完成，而是关注如何使用有限的资源，尽可能在单位时间内处理更多的任务，也就是吞吐量优先的问题。所以应该设置队列去缓冲并发任务，调整合适的corePoolSize去设置处理任务的线程数。在这里，设置的线程数过多可能还会引发线程上下文切换频繁的问题，也会降低处理任务的速度，降低吞吐量。

# execute和submit区别
execute和submit都属于线程池的方法，execute只能提交Runnable类型的任务，而submit既能提交Runnable类型任务也能提交Callable类型任务。 execute会直接抛出任务执行时的异常，submit会吃掉异常，可通过Future的get方法将任务执行时的异常重新抛出。

# 死锁
```java
public class DeadLockDemo {
    private static Object resource1 = new Object();//资源 1 
    private static Object resource2 = new Object();//资源 2

    public static void main(String[] args) {
        new Thread(()->{
            synchronized(resource1){
                System.out.println(Thread.currentThread() + "get resource1");
                try{
                    Thread.sleep(1000);
                } catch(InterruptedException e){
                    e.printStackTrace();
                }
                System.out.println(Thread.currentThread() + "waiting get resource2");
                synchronized(resource2){
                    System.out.println(Thread.currentThread() + "get resource2");
                }
            }
        }, "Thread 1").start();

        new Thread(()->{
            synchronized(resource2){
                System.out.println(Thread.currentThread() + "get resource2");
                try{
                    Thread.sleep(1000);
                } catch(InterruptedException e){
                    e.printStackTrace();
                }
                System.out.println(Thread.currentThread() + "waiting get resource1");
                synchronized(resource1){
                    System.out.println(Thread.currentThread() + "get resource1");
                }
            }
        }, "Thread 2").start();
    }
}
```
output
```
Thread[线程 1,5,main]get resource1
Thread[线程 2,5,main]get resource2
Thread[线程 1,5,main]waiting get resource2
Thread[线程 2,5,main]waiting get resource1
```
 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过 Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视 器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等
待的状态，这也就产生了死锁。上面的例子符合产生死锁的四个必要条件。

解决
```java
// 对线程2代码修改
new Thread(() -> {
    synchronized (resource1) {
        System.out.println(Thread.currentThread() + "get resource1");
        try {
            Thread.sleep(1000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println(Thread.currentThread() + "waiting get resource2");
        synchronized (resource2) {
            System.out.println(Thread.currentThread() + "get resource2");
        }
}
```


# JMM

JMM理解：Java内存模型规范了Jvm解决并发过程中的有序性、原子性、可见性问题的方法

- 可见性：一个线程对共享变量的修改，能够让其他线程立刻感知到
- 有序性：程序按照代码顺序执行
- 原子性：一个或多个操作要么都执行，要么都不执行

## synchronized

### 作用

对象锁：修饰`this`、成员方法、成员变量

类锁：修饰`*.class`、静态方法

### 底层原理

synchronized编译成字节码为`monitorenter`（加锁）和`monitorexit`（释放锁），依赖于底层的操作系统的Mutex Lock来实现

### JVM锁优化

monitorenter和monitorexit字节码是依赖操作系统的mutex lock来实现，由于线程竞争锁失败时，使用mutex lock需要将当前线程挂起（触发系统调用），并且从用户态切换到内核态执行，切换的开销很大；而在现实中大部分情况下，同步方法都是在单线程环境（无锁竞争环境）运行，每次都调用mutex lock会严重影响性能，因此引入锁优化。

#### synchronized同步锁的类型

- 无锁
- 偏向锁
- 轻量级锁
- 重量级锁

```shell
锁膨胀方向：无锁->偏向锁->轻量级锁->重量级锁（过程不可逆）
```

#### 锁优化方式

##### 自旋锁与自适应自旋锁

当一个线程获取锁失败时，会挂起当前线程，在后续获取锁成功后再恢复线程，这些操作都需要切换到内核态完成，影响系统并发性能；而在很多情况下，共享数据的锁定状态只会持续很短一段时间，为了这段时间去挂起和恢复阻塞线程并不值得，在多cpu环境下，完全可以让没有获取到锁的线程自旋（忙循环）等待一会，即占用cpu时间片

如果所占用时间非常短，那么自旋锁的性能会非常好，相反会带来更多的性能开销，因为在线程自旋时，始终占用cpu时间片，如果锁占用时间太长，那么自旋的线程会白白消耗掉cpu资源，用户可通过`-XX:PreBlockSpin`修改自旋次数。

在JDK1.6中引入了自适应自旋锁，意味着自旋的次数不再固定。对于同一个锁对象，如果通过自旋等待成功获取过该锁，则JVM任务该锁自旋获取到锁的可能性很大，会自动增加等待时间；相反，如果对于某个锁，很少通过自旋成功获取锁，那以后要获取这个锁时可能会省略掉自旋过程，以避免浪费处理器资源。

##### 锁消除

在JIT运行时，通过逃逸分析判断数据不会逃逸出去被其他线程访问，就会认为这些数据是线程私有的，不需要加同步锁，此时会进行锁消除

例子：由于String是不可变类，对字符串的拼接操作总是通过生成新的String对象进行，编译器会做自动优化，在jdk1.5前，会使用StringBuffer的连续append操作，在1.5之后，转化为StringBuilder的连续append操作。

##### 锁粗化

一般加同步锁的时候，会尽量减小作用范围，一是为了尽量减少需要同步操作的数量，二是为了让等待锁的线程尽快能获取到锁。但如果存在连串的一系列操作对同一个对象反复加锁解锁，即使没有锁竞争，频繁的进行互斥同步操作也会导致性能下降。

例子：

```java
public static String test04(String s1, String s2, String s3) {
    StringBuffer sb = new StringBuffer();
    sb.append(s1);
    sb.append(s2);
    sb.append(s3);
    return sb.toString();
}
```

JVM会检测到一连串操作都是对同一个对象加锁，就会将加锁同步的范围扩展（粗化）到整个一系列操作的外部，使得只需加一次锁。

##### 轻量级锁

引入背景：在大多数情况下同步块并不会有竞争出现

> 对象头分为mark word（存储对象自身运行时数据，如HashCode，GC Age，锁标记位等）和klass point（指向方法区类元数据的指针），如果对象是数组，还会有一个额外的部分用于存储数据的长度。

在线程执行同步块之前，jvm会先在当前线程的栈帧中创建名为锁记录（Lock Record）的空间，用于存储锁对象目前的mark word的拷贝（官方称为Displaced Mark Word）

如果当前对象没有被锁定，则锁标记位为01状态

然后jvm会使用CAS操作将mark word的内容拷贝到当前线程的lock record中，并且将mark word更新为指向lock record的指针。如果更新成功，则当前线程拥有了该对象锁，并且对象mark word的锁标记位更新为00，表示此对象处于轻量级锁定状态。

如果更新操作失败，jvm会检查当前mark word是否存在指向当前线程的栈帧的指针，如果有，则说明锁已被获取，可以直接调用，如果没有，则说明锁被其他线程抢占，如果有两个以上的线程竞争同一把锁，那轻量级锁则失效，直接膨胀为重量级锁，膨胀过程中，JVM会创建一个ObjectMonitor结构（可能预先存在也可能即时创建），mark word存储指向该结构的指针，最后两位标志位设置为10，没有获取锁的线程被阻塞

轻量级锁解锁时，通过CAS操作将Displaced Mark Word替换回到对象头中，如果成功，说明没有竞争关系，如果失败，表示当前锁存在竞争关系，锁已膨胀成重量级锁（有多个线程尝试获取），于是会执行重量级锁解锁

##### 偏向锁

引入背景：大多数情况下，不仅不存在多线程竞争锁，而且总是由同一个线程多次获取同一个锁。

一旦锁被某个线程获得，JVM会“偏向”这个线程，后续在无竞争的情况下，该线程进入和退出同步块时几乎不需要执行任何同步操作（如CAS）

目的：消除一个线程反复进入同一同步块时的同步开销（如CAS、更新Lock Record）

![image-20250605130533646](../img/多线程/image-20250605130533646.png)

偏向锁过程

1、对象初始化状态（可偏向但未偏）

- ​	Mark Word：biased_lock = 1（可偏向），lock = 01， Thread ID = 0（尚未偏向任何线程）
- 此时对象是可偏向但无主的（Anonymous Biased）

2、获取偏向锁（第一次进入同步块）

- 线程T1进入synchronized(obj)块

- JVM通过CAS操作，把对象的Mark Word中的Thread ID改为当前线程T1的指针（即当前线程的`JavaThread*`地址），而非线程ID

- CAS成功：

 - Mark Word变为：biased_lock=1，lock = 01，Thread ID = T1_ID，age = x
 - 此时不需要在T1的栈帧中创建Lock Record或拷贝Displaced Mark Word（这是与轻量级锁的关键区别）

- CAS失败：

 - 原因1：并发导致CAS失败（说明有其他线程也尝试获取偏向锁，出现竞争），触发偏向锁撤销

 - 原因2：该对象的identity hash code已被计算（首次调用 `hashCode()` 或 `System.identityHashCode()` 时生成并写入 Mark Word），identity hash code和线程ID之间互斥（Mark Word要么存放identity hash code，要么存放线程ID），此时触发撤销偏向锁+永久禁用该对象的偏向锁机制

   ```shell
   在32位JVM中，Mark Word结构示例：
   | 25 bits: identity hashcode | 4 bits: age | 1 bit: biased_lock | 2 bits: lock flag |
   在64位JVM中（开启压缩指针）：
   | 31 bits: identity hashcode | 1 bit: unused | 4 bits: age | 1 bit: biased_lock | 2 bits
   ```

3、持有偏向锁的线程重入同步块

- JVM只需检查对象Mark Word中的Thread ID是否指向T1
- 若指向T1，则线程T1直接进入同步块，不需要任何CAS或更新操作，JVM在T1的栈帧中创建Lock Record，但其Displaced Mark Word设置为NULL（或特定标记），仅用于记录重入次数
- 解锁时，Lock Record被弹出，不会尝试CAS还原Mark Word（因为Mark Word仍保持为偏向T1状态）

4、撤销偏向锁

- 触发时机：
 - 竞争出现：另一个线程T2尝试进入synchronized(obj)，检测到锁当前偏向于T1
 - 需要计算Hash，在线程T1持有偏向锁期间，对该对象调用了System.identityHashCode()
 - 批量撤销：JVM检测到某类的对象普遍存在竞争，触发对该类所有实例的偏向锁撤销（使用epoch机制优化）

##### 偏向锁的淘汰原因（补充说明）

1. **撤销开销：** 在竞争出现时，撤销偏向锁操作代价高昂，通常需要全局安全点或复杂同步。
2. **短暂对象：** 大量 Java 对象（尤其在 Web 应用中）生命周期很短，可能在其生命周期内根本来不及偏向某个线程，或者偏向后马上又因竞争而撤销，得不偿失。
3. **维护成本：** `epoch` 机制引入了一定的复杂性。
4. **硬件优化：** 现代 CPU 的 CAS 操作在无竞争时已经非常高效，使得轻量级锁的开销本身就很低。偏向锁带来的微小增益（仅省去了单次重入时的 CAS）相比之下显得不够吸引人。
5. **用户混淆：** 偏向锁的行为有时难以预测和理解，容易导致性能问题排查困难。

### synchronized的缺陷

- 效率低：高竞争场景下，性能劣化（升级重量锁）
- 不够灵活：只有代码执行完毕或异常结束才会释放锁，不能设置超时
- 无法知道是否成功获取锁

### 使用synchronized注意事项

- 锁对象不能为空，因为锁的信息都在对象头
- 作用域不能过大，影响程序执行的速度
- 避免死锁
- 在能选择的情况下，既不要用Lock也不要用synchronized，用concurrent包中各种类；如果不用该包下的类，在满足业务情况下，可以使用synchronized，因为代码量少，避免出错
- synchronized是非公平锁，新来的线程有可能立即获得监视器，而在等待区中等待已久的线程可能再次等待，这样有利于提高性能，但是也可能会导致饥饿现象

## Volatile

### 关键作用

- 防止指令重排
- 保证可见性
- 不能保证完全的原子性，只能保证单次的读/写操作具有原子性
 - 比如i++不能保证原子性，因为i++包括三步骤
  - 读取i的值
  - 对i加1
  - 将i的值写回内存

### 实现原理

#### 可见性实现

Volatile变量的内存可见性是基于内存屏障（一个CPU lock指令）实现

如果对声明了volatile的变量进行写操作，JVM会向处理器发送一条lock前缀的指令，该变量所在缓存行（一般64byte）的数据会被写回主内存。

根据缓存一致性原理，每个处理器通过总线嗅探检查到被修改的变量的内存地址在自己的缓存行中，就会将当前的缓存行设置为invalid，下次读取该变量就会重新从主内存中读取。

#### 有序性实现

- happens-before规则

- 在指令中插入内存屏障

 - 在每个volatile写操作前面插入StoreStore屏障

 - 在每个volatile写操作后面插入StoreLoad屏障

 - 在每个volatile读操作后面插入LoadLoad屏障

 - 在每个volatile读操作后面插入LoadStore屏障

   | 内存配置       | 说明                                                  |
       | -------------- | ----------------------------------------------------- |
   | StoreStore屏障 | 禁止上面的普通写和下面的volatile写重排序              |
   | StoreLoad屏障  | 禁止上面的volatile写和下面可能有的volatile读/写重排序 |
   | LoadLoad屏障   | 禁止下面所有的普通读操作和上面的volatile读重排序      |
   | LoadStore屏障  | 禁止下面所有的普通写操作和上面的volatile读重排序      |

当且仅当满足以下所有条件时使用 `volatile`：

1. 变量状态**独立于其他变量**
2. 写操作**不依赖当前值**
3. 只需要**可见性**而**不需要原子性**
4. **读操作频率**远高于写操作